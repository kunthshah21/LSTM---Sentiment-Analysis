{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319b1b9f",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503da4c6-8c0c-4ef8-bc96-016911392e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd85f33-807a-4506-9121-26c19e4fdd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/tweet_emotions.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9f09e",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807a485",
   "metadata": {},
   "source": [
    "## Removing ID, duplicate and Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2600622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   40000 non-null  int64 \n",
      " 1   sentiment  40000 non-null  object\n",
      " 2   content    40000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#If there is any null value throughout the row, remove it.\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef618d1c",
   "metadata": {},
   "source": [
    "It was noticed that there are no null values present, therefore no values were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb349a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the id column\n",
    "data = data.drop(['tweet_id'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bd8ef",
   "metadata": {},
   "source": [
    "### Removing Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6d615",
   "metadata": {},
   "source": [
    "In this step, duplicate rows based on the \"content\" column were identified and removed. Duplicate rows can introduce bias and redundancy in the dataset, which can negatively impact the performance of machine learning models. By removing these duplicates, each tweet is ensured to be unique, leading to a more accurate and reliable analysis.\n",
    "\n",
    "The process involved:\n",
    "1. **Counting Duplicates**: The number of duplicate rows in the dataset was counted.\n",
    "2. **Removing Duplicates**: The duplicate rows based on the \"content\" column were removed.\n",
    "3. **Resetting Index**: After removing duplicates, the index of the dataframe was reset to maintain a clean and sequential index.\n",
    "\n",
    "The code used for this process is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10b1446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows before removing:  91\n",
      "Number of duplicate rows after removing:  0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplciate rows if they have the same \"content\" value. Also print the number of removed rows.\n",
    "print(\"Number of duplicate rows before removing: \", data.duplicated().sum())\n",
    "data = data.drop_duplicates(subset='content')\n",
    "data = data.reset_index(drop=True)\n",
    "print(\"Number of duplicate rows after removing: \", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d17f37",
   "metadata": {},
   "source": [
    "There were 91 duplicate rows found, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04f4dc",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb929bc7",
   "metadata": {},
   "source": [
    "The text cleaning function is designed to preprocess and clean the text data in the dataset. It performs the following operations:\n",
    "\n",
    "1. **Remove URLs**: Eliminates any URLs from the text.\n",
    "2. **Remove Non-Word Characters**: Replaces non-word characters with spaces.\n",
    "3. **Remove @Mentions**: Removes mentions (e.g., `@username`).\n",
    "4. **Remove Hashtags**: Removes the `#` symbol from hashtags.\n",
    "5. **Remove Non-ASCII Characters**: Removes any non-ASCII characters.\n",
    "6. **Remove Digits**: Eliminates digits from the text.\n",
    "7. **Fix Multiple Spaces**: Replaces multiple spaces with a single space.\n",
    "8. **Trim Spaces**: Removes leading and trailing spaces.\n",
    "9. **Lemenisation** Applied word net lemmenisation to all words\n",
    "\n",
    "The function is applied to the `content` column of the dataset to standardize the text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fb3ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                            content\n",
      "0       empty  tiffanylue i know i wa listenin to bad habit e...\n",
      "1     sadness  layin n bed with a headache ughhhh waitin on y...\n",
      "2     sadness                     funeral ceremony gloomy friday\n",
      "3  enthusiasm                  want to hang out with friend soon\n",
      "4     neutral  dannycastillo we want to trade with someone wh...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources (run only once)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    #remove any @mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    #remove # from #hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    #remove any non-ascii characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    #remove any digits\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    \n",
    "    # Fix double or multiple spacing cause from removal\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove any leading or trailing spaces\n",
    "    text = re.sub(r'^\\s+|\\s+?$', '', text)\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the function to the \"content\" column\n",
    "data['content'] = data['content'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead12d56",
   "metadata": {},
   "source": [
    "Saving the cleaned data to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e689193",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dataset/cleaned_tweet_emotions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eadeeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue i know i wa listenin to bad habit e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache ughhhh waitin on y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want to hang out with friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo we want to trade with someone wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  tiffanylue i know i wa listenin to bad habit e...\n",
       "1     sadness  layin n bed with a headache ughhhh waitin on y...\n",
       "2     sadness                     funeral ceremony gloomy friday\n",
       "3  enthusiasm                  want to hang out with friend soon\n",
       "4     neutral  dannycastillo we want to trade with someone wh..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/cleaned_tweet_emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "324b571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39827, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "content      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9cb3c",
   "metadata": {},
   "source": [
    "## Embedding Tweets with GloVe and TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5161f3",
   "metadata": {},
   "source": [
    "\n",
    "In this section, tweets were embedded using a combination of GloVe embeddings and TF-IDF weighting. GloVe (Global Vectors for Word Representation) is a pre-trained word embedding model that captures semantic relationships between words. The 200-dimensional GloVe embeddings trained on Twitter data were used, which is particularly suitable for the sentiment analysis task on tweets due to its training corpus.\n",
    "\n",
    "The process involved:\n",
    "1. **Loading GloVe Embeddings**: The pre-trained GloVe embeddings were loaded from a file.\n",
    "2. **TF-IDF Vectorization**: A TF-IDF vectorizer was fitted on the dataset to capture the importance of words in the context of the specific dataset.\n",
    "3. **Combining GloVe and TF-IDF**: For each tweet, a weighted average of the GloVe vectors of its words was computed, using the TF-IDF scores as weights. This resulted in a single 200-dimensional vector representing each tweet.\n",
    "\n",
    "This approach leverages the semantic richness of GloVe embeddings and the contextual relevance captured by TF-IDF, providing a robust representation of tweets for sentiment analysis. Compared to other encodings, this method is particularly effective for short, informal text like tweets, where word context and importance can vary significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dddece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                            content  \\\n",
      "0       empty  tiffanylue i know i wa listenin to bad habit e...   \n",
      "1     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
      "2     sadness                     funeral ceremony gloomy friday   \n",
      "3  enthusiasm                  want to hang out with friend soon   \n",
      "4     neutral  dannycastillo we want to trade with someone wh...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.009336741176117474, -0.11781357224288551, ...  \n",
      "1  [0.020380503826825182, -0.05467616021290979, -...  \n",
      "2  [-0.10435475312059417, -0.02760633744912397, -...  \n",
      "3  [0.0272797092070235, 0.18452336164318683, 0.01...  \n",
      "4  [0.02917883614275043, 0.36351611138752765, -0....  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load Pre-trained GloVe Embeddings (200d)\n",
    "def load_glove_embeddings(filepath):\n",
    "    glove_dict = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_dict[word] = vector\n",
    "    return glove_dict\n",
    "\n",
    "# Convert tweets to TF-IDF weighted GloVe embeddings\n",
    "def get_tweet_embedding(tweet, glove_dict, tfidf, feature_names):\n",
    "    words = tweet.split()\n",
    "    tweet_vector = np.zeros(200)  # GloVe 200d\n",
    "    word_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in glove_dict and word in feature_names:\n",
    "            weight = tfidf.get(word, 1)  # Default to 1 if word not in TF-IDF dict\n",
    "            tweet_vector += weight * glove_dict[word]\n",
    "            word_count += weight\n",
    "\n",
    "    return tweet_vector / word_count if word_count != 0 else tweet_vector\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "glove_path = \"app/glove.twitter.27B.200d.txt\"\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "# TF-IDF Vectorizer (Fitted on Your Dataset), then pickle it\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_vectorizer.fit(df['content'])\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "# Load the saved TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "feature_names = set(loaded_tfidf_vectorizer.get_feature_names_out())\n",
    "idf_scores = dict(zip(loaded_tfidf_vectorizer.get_feature_names_out(), loaded_tfidf_vectorizer.idf_))\n",
    "\n",
    "# Convert Tweets to GloVe Embeddings\n",
    "df['embedding'] = df['content'].apply(lambda x: get_tweet_embedding(x, glove_embeddings, idf_scores, feature_names))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349330b",
   "metadata": {},
   "source": [
    "### Label encoding for the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f4fe6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n",
      " 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"
     ]
    }
   ],
   "source": [
    "#print all unique values in the sentiment column\n",
    "print(df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b05a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                            content  \\\n",
      "0       empty  tiffanylue i know i wa listenin to bad habit e...   \n",
      "1     sadness  layin n bed with a headache ughhhh waitin on y...   \n",
      "2     sadness                     funeral ceremony gloomy friday   \n",
      "3  enthusiasm                  want to hang out with friend soon   \n",
      "4     neutral  dannycastillo we want to trade with someone wh...   \n",
      "\n",
      "                                           embedding  sentiment_encoded  \n",
      "0  [-0.009336741176117474, -0.11781357224288551, ...                  0  \n",
      "1  [0.020380503826825182, -0.05467616021290979, -...                  0  \n",
      "2  [-0.10435475312059417, -0.02760633744912397, -...                  0  \n",
      "3  [0.0272797092070235, 0.18452336164318683, 0.01...                  2  \n",
      "4  [0.02917883614275043, 0.36351611138752765, -0....                  1  \n",
      "sentiment_encoded\n",
      "0    16023\n",
      "2    15205\n",
      "1     8598\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Perform label encoding in a new colomn. [empty, sadness, worry, hate, boredom, anger] is the first label which is negative. [neutral] label is neutral. [enthusiasm, love, fun, happiness, relief] is positive label. Create a new colomn for encoding.\n",
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x: 0 if x in ['empty', 'sadness', 'worry', 'hate', 'boredom', 'anger'] else 1 if x in ['neutral'] else 2)\n",
    "print(df.head())\n",
    "\n",
    "#print how many instances are there in each class\n",
    "print(df['sentiment_encoded'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11212f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39826, 4)\n",
      "sentiment            object\n",
      "content              object\n",
      "embedding            object\n",
      "sentiment_encoded     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab399a",
   "metadata": {},
   "source": [
    "## Further Data Pre-processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97a5d0",
   "metadata": {},
   "source": [
    "It was noticed that once the encoding was done, there were some issues with how the data was stored and how it was retrieved. This caused the model to not run since it can not read strings, therefore the data was further processed to change the data type and remove trailing and beginning text that was added during time of encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce8f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def parse_embedding(embedding):\n",
    "    # 1) If it's already a NumPy array, just ensure dtype float32.\n",
    "    if isinstance(embedding, np.ndarray):\n",
    "        return embedding.astype(np.float32)\n",
    "    \n",
    "    # 2) If it's a string that looks like \"array([-0.07, 0.08, ...])\", remove \"array(\" and trailing \")\".\n",
    "    if isinstance(embedding, str):\n",
    "        if embedding.startswith(\"array(\") and embedding.endswith(\")\"):\n",
    "            # remove the leading array( and trailing )\n",
    "            embedding = embedding[len(\"array(\"):-1]  # everything inside the parentheses\n",
    "\n",
    "        # now it should look like \"[-0.07, 0.08, ...]\"\n",
    "        python_list = ast.literal_eval(embedding)  # parse as Python list\n",
    "        return np.array(python_list, dtype=np.float32)\n",
    "    \n",
    "    # 3) Otherwise, try to convert it to float32 array anyway (covers lists or other formats).\n",
    "    return np.array(embedding, dtype=np.float32)\n",
    "\n",
    "# Now apply\n",
    "df['embedding'] = df['embedding'].apply(parse_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0804f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.33674164e-03 -1.17813572e-01  1.22281229e-02  6.81492612e-02\n",
      " -6.33047298e-02  1.30258262e-01  5.25684357e-01 -2.91683655e-02\n",
      " -3.08205783e-01 -3.26570541e-01 -2.42037419e-02 -1.09019227e-01\n",
      " -6.26551151e-01 -1.21132649e-01 -1.47696985e-02 -2.19138026e-01\n",
      "  1.58629134e-01  1.12198010e-01 -9.28363428e-02 -8.19794461e-02\n",
      "  6.53500184e-02  1.51154352e-02  1.46863945e-02 -1.94295198e-01\n",
      " -7.14265183e-02  9.73988235e-01 -1.12449199e-01  7.18857348e-02\n",
      " -5.75402007e-03 -9.51587930e-02 -1.14354961e-01 -7.59564415e-02\n",
      " -3.50154161e-01  4.51495834e-02 -2.32437313e-01  1.06302112e-01\n",
      " -2.54962463e-02 -1.25372291e-01  2.15248689e-01  5.58137894e-02\n",
      "  4.61367577e-01  2.26965860e-01  1.52214497e-01 -5.74009120e-02\n",
      " -1.53786600e-01  1.57505050e-02  4.47052896e-01 -2.52260659e-02\n",
      " -3.82465087e-02  1.31910861e-01  8.63551274e-02 -2.89009154e-01\n",
      " -6.92554861e-02 -1.41300857e-01  2.20181625e-02  1.73297785e-02\n",
      " -7.28377327e-02 -1.81020528e-01  1.54833734e-01 -1.40639067e-01\n",
      "  3.45269032e-02  3.43265459e-02 -9.47643965e-02  7.06314668e-02\n",
      " -6.21679798e-02 -4.14117053e-02  5.93353361e-02  1.37479901e-01\n",
      "  4.76292036e-02 -1.37709454e-01  7.00411759e-03  7.74258822e-02\n",
      " -1.19492114e-01 -3.94446626e-02  1.63087517e-01  1.04754575e-01\n",
      "  1.10182352e-01 -4.87980321e-02 -7.67408758e-02  1.27572194e-01\n",
      "  1.04788445e-01  6.76316172e-02 -1.57089457e-01  1.02598406e-01\n",
      "  2.56418921e-02  1.66557968e-01  3.52738542e-03 -2.52549976e-01\n",
      "  5.89803159e-02 -6.67197332e-02  1.42131180e-01  5.51136248e-02\n",
      " -5.35480790e-02 -7.33075961e-02  9.44732055e-02  1.42643079e-01\n",
      " -4.70472127e-03  9.48750749e-02  3.91130783e-02 -1.75840259e-02\n",
      " -5.05606569e-02 -2.81404555e-01 -2.05171872e-02  6.33670837e-02\n",
      " -7.70057812e-02  7.37892166e-02  1.25231981e-01 -4.95759770e-02\n",
      " -8.32373723e-02  2.37963684e-02 -8.59214664e-02 -1.51001826e-01\n",
      "  2.90769581e-02  1.78534657e-01 -2.56962985e-01 -3.76234390e-02\n",
      "  1.61375538e-01  1.18826658e-01 -4.46421877e-02  9.45530646e-03\n",
      "  6.06152676e-02 -2.13389948e-01  3.28479826e-01 -1.22765481e-01\n",
      " -1.23721443e-01  2.22314417e-01  7.03159869e-02 -6.89108148e-02\n",
      "  2.60979027e-01 -2.45800931e-02 -2.04983577e-02  3.03728115e-02\n",
      " -2.57474370e-02 -4.84308302e-02 -2.22663969e-01 -2.21152246e-01\n",
      " -1.08084947e-01  6.59150910e-03 -9.41419229e-02  1.22209147e-01\n",
      " -1.74320620e-02  2.06072807e-01  1.50483726e-02 -1.97044853e-02\n",
      " -1.18776403e-01 -2.63151657e-02 -6.13972023e-02 -1.13368943e-01\n",
      "  1.95312738e-01 -1.30469501e-01  1.34637609e-01  1.09784074e-01\n",
      " -4.05238724e+00  1.75711904e-02 -1.47450045e-01 -3.36322151e-02\n",
      "  7.96980336e-02 -3.35419886e-02 -4.19749916e-02  3.38716172e-02\n",
      "  1.51939839e-01  4.46775034e-02  1.04155965e-01  4.91708033e-02\n",
      "  1.31989419e-01 -2.28888448e-02  2.57095426e-01  7.77980685e-02\n",
      " -5.90481237e-02 -1.31968051e-01  1.83515474e-02 -9.93677974e-02\n",
      " -8.40914845e-02 -1.26732960e-01 -5.81069067e-02 -6.22203434e-03\n",
      " -1.08793370e-01  8.13338459e-02 -8.21705386e-02  5.67726791e-02\n",
      " -1.28612235e-01  1.15730464e-01  2.77174474e-03  4.62203212e-02\n",
      "  1.48511529e-01  9.44164023e-02  2.48522460e-01 -7.22969249e-02\n",
      "  9.05098990e-02 -2.89187133e-02 -6.00454062e-02 -1.89269915e-01\n",
      "  8.60669985e-02  1.15876600e-01 -1.41615793e-02 -2.83501260e-02\n",
      "  1.12531230e-01  3.53604287e-01 -1.22958459e-01 -5.87146766e-02]\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(df['embedding'].iloc[0])\n",
    "print(type(df['embedding'].iloc[0]))  # <class 'numpy.ndarray'>\n",
    "print(df['embedding'].iloc[0].dtype)  # float32 (or float64, depending on your code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6bc5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           embedding  sentiment_encoded\n",
      "0  [-0.009336742, -0.11781357, 0.012228123, 0.068...                  0\n",
      "1  [0.020380504, -0.05467616, -0.048116226, -0.20...                  0\n",
      "2  [-0.104354754, -0.027606338, -0.0489604, 0.226...                  0\n",
      "3  [0.027279709, 0.18452336, 0.019765332, -0.4269...                  2\n",
      "4  [0.029178835, 0.36351612, -0.011995873, 0.0128...                  1\n"
     ]
    }
   ],
   "source": [
    "#Store only the embedding colomn and the sentiment_encoded colomn in a new dataframe.\n",
    "df = df[['embedding', 'sentiment_encoded']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53c222e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embedding            object\n",
       "sentiment_encoded     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149c97b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to 'embeddings_as_string.csv'\n"
     ]
    }
   ],
   "source": [
    "df[\"embedding\"] = df[\"embedding\"].apply(lambda arr: arr.tolist())\n",
    "\n",
    "#Save to CSV\n",
    "df.to_csv(\"dataset/embeddings_as_string.csv\", index=False)\n",
    "print(\"\\nSaved to 'embeddings_as_string.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fbe969f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.009336741641163826, -0.11781357228755951, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.020380504429340363, -0.05467616021633148, -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.10435475409030914, -0.027606338262557983, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.02727970853447914, 0.18452335894107819, 0.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.029178835451602936, 0.3635161221027374, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           embedding  sentiment_encoded\n",
       "0  [-0.009336741641163826, -0.11781357228755951, ...                  0\n",
       "1  [0.020380504429340363, -0.05467616021633148, -...                  0\n",
       "2  [-0.10435475409030914, -0.027606338262557983, ...                  0\n",
       "3  [0.02727970853447914, 0.18452335894107819, 0.0...                  2\n",
       "4  [0.029178835451602936, 0.3635161221027374, -0....                  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f0129",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a426644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 27878\n",
      "Validation set size: 7965\n",
      "Test set size: 3983\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train (70%) and temp (30%) with stratification\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['sentiment_encoded'])\n",
    "\n",
    "# Split the temp dataset into validation (20% of original) and test (10% of original) with stratification\n",
    "val_df, test_df = train_test_split(temp_df, test_size=1/3, random_state=42, stratify=temp_df['sentiment_encoded'])\n",
    "\n",
    "# Print the sizes of the splits to verify\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08dd5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_lstm_data(df, label_col='sentiment_encoded', embed_col='embedding'):\n",
    "    \"\"\"\n",
    "    df:       DataFrame with at least 2 columns: [label_col, embed_col]\n",
    "    label_col: name of the sentiment/label column\n",
    "    embed_col: name of the embedding column (a numerical vector or numeric data)\n",
    "    \"\"\"\n",
    "    # 1) Extract labels\n",
    "    y = df[label_col].values  # shape -> (num_samples,)\n",
    "\n",
    "    # 2) Extract numeric features (assuming 'embedding' column contains numeric vectors)\n",
    "    #    If 'embedding' is already stored as a vector (list/np.array) per row, convert each row to np.array:\n",
    "    X = np.array(df[embed_col].tolist())  # shape -> (num_samples, embedding_dim)\n",
    "\n",
    "    # 3) Reshape to 3D for LSTM: (samples, timesteps=1, features=embedding_dim)\n",
    "    #    If each row is just one “step” with that embedding:\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db74e92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27878, 1, 200)\n",
      "y_train shape: (27878,)\n",
      "X_val shape: (7965, 1, 200)\n",
      "y_val shape: (7965,)\n",
      "X_test shape: (3983, 1, 200)\n",
      "y_test shape: (3983,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Keras / TensorFlow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Sklearn for additional metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "X_train, y_train = prepare_lstm_data(train_df,\n",
    "                                     label_col='sentiment_encoded',\n",
    "                                     embed_col='embedding')\n",
    "\n",
    "X_val, y_val = prepare_lstm_data(val_df,\n",
    "                                 label_col='sentiment_encoded',\n",
    "                                 embed_col='embedding')\n",
    "\n",
    "X_test, y_test = prepare_lstm_data(test_df,\n",
    "                                   label_col='sentiment_encoded',\n",
    "                                   embed_col='embedding')\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (28000, 1, embedding_dim) for example\n",
    "print(\"y_train shape:\", y_train.shape)  # (28000,)\n",
    "\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f24dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers, initializers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(64, \n",
    "                  return_sequences=True, \n",
    "                  kernel_initializer=initializers.GlorotUniform(),\n",
    "                  recurrent_initializer=initializers.Orthogonal(),\n",
    "                  bias_initializer='zeros',\n",
    "                  kernel_regularizer=regularizers.l2(1e-4), \n",
    "                  input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Second LSTM layer - new\n",
    "    model.add(LSTM(48, \n",
    "                  return_sequences=True, \n",
    "                  kernel_initializer=initializers.GlorotUniform(),\n",
    "                  recurrent_initializer=initializers.Orthogonal(),\n",
    "                  bias_initializer='zeros',\n",
    "                  kernel_regularizer=regularizers.l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Third LSTM layer - new\n",
    "    model.add(LSTM(32, \n",
    "                  return_sequences=True, \n",
    "                  kernel_initializer=initializers.GlorotUniform(),\n",
    "                  recurrent_initializer=initializers.Orthogonal(),\n",
    "                  bias_initializer='zeros',\n",
    "                  kernel_regularizer=regularizers.l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Fourth LSTM layer (final)\n",
    "    model.add(LSTM(16, \n",
    "                  return_sequences=False, \n",
    "                  kernel_initializer=initializers.GlorotUniform(),\n",
    "                  recurrent_initializer=initializers.Orthogonal(),\n",
    "                  bias_initializer='zeros',\n",
    "                  kernel_regularizer=regularizers.l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense output layer\n",
    "    model.add(Dense(3, \n",
    "                   activation='softmax', \n",
    "                   kernel_initializer=initializers.GlorotUniform(),\n",
    "                   bias_initializer='zeros',\n",
    "                   kernel_regularizer=regularizers.l2(1e-4)))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aad892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,840</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,696</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m67,840\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m48\u001b[0m)          │        \u001b[38;5;34m21,696\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m48\u001b[0m)          │           \u001b[38;5;34m192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m48\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │        \u001b[38;5;34m10,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">103,731</span> (405.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m103,731\u001b[0m (405.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">103,411</span> (403.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m103,411\u001b[0m (403.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> (1.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m320\u001b[0m (1.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: X_train.shape[1:] is (timesteps, features)\n",
    "model = build_lstm_model(X_train.shape[1:])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cfcbb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - AUC: 0.5388 - loss: 1.4229 - val_AUC: 0.7063 - val_loss: 1.0442\n",
      "Epoch 2/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.6046 - loss: 1.2318 - val_AUC: 0.7151 - val_loss: 1.0104\n",
      "Epoch 3/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.6403 - loss: 1.1513 - val_AUC: 0.7274 - val_loss: 0.9931\n",
      "Epoch 4/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.6645 - loss: 1.1044 - val_AUC: 0.7383 - val_loss: 0.9766\n",
      "Epoch 5/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.6806 - loss: 1.0757 - val_AUC: 0.7450 - val_loss: 0.9659\n",
      "Epoch 6/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.6860 - loss: 1.0606 - val_AUC: 0.7500 - val_loss: 0.9596\n",
      "Epoch 7/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7006 - loss: 1.0385 - val_AUC: 0.7536 - val_loss: 0.9539\n",
      "Epoch 8/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7062 - loss: 1.0281 - val_AUC: 0.7566 - val_loss: 0.9489\n",
      "Epoch 9/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7086 - loss: 1.0232 - val_AUC: 0.7593 - val_loss: 0.9448\n",
      "Epoch 10/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7157 - loss: 1.0101 - val_AUC: 0.7610 - val_loss: 0.9423\n",
      "Epoch 11/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7315 - loss: 0.9855 - val_AUC: 0.7633 - val_loss: 0.9386\n",
      "Epoch 12/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7336 - loss: 0.9833 - val_AUC: 0.7644 - val_loss: 0.9364\n",
      "Epoch 13/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7337 - loss: 0.9830 - val_AUC: 0.7664 - val_loss: 0.9329\n",
      "Epoch 14/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7369 - loss: 0.9776 - val_AUC: 0.7675 - val_loss: 0.9312\n",
      "Epoch 15/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7385 - loss: 0.9757 - val_AUC: 0.7684 - val_loss: 0.9295\n",
      "Epoch 16/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7427 - loss: 0.9692 - val_AUC: 0.7693 - val_loss: 0.9278\n",
      "Epoch 17/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7508 - loss: 0.9560 - val_AUC: 0.7699 - val_loss: 0.9266\n",
      "Epoch 18/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7478 - loss: 0.9608 - val_AUC: 0.7714 - val_loss: 0.9240\n",
      "Epoch 19/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7531 - loss: 0.9515 - val_AUC: 0.7718 - val_loss: 0.9227\n",
      "Epoch 20/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7567 - loss: 0.9459 - val_AUC: 0.7719 - val_loss: 0.9226\n",
      "Epoch 21/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7514 - loss: 0.9538 - val_AUC: 0.7727 - val_loss: 0.9221\n",
      "Epoch 22/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7552 - loss: 0.9475 - val_AUC: 0.7731 - val_loss: 0.9205\n",
      "Epoch 23/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7595 - loss: 0.9412 - val_AUC: 0.7736 - val_loss: 0.9200\n",
      "Epoch 24/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7600 - loss: 0.9398 - val_AUC: 0.7740 - val_loss: 0.9180\n",
      "Epoch 25/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7618 - loss: 0.9382 - val_AUC: 0.7752 - val_loss: 0.9160\n",
      "Epoch 26/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7644 - loss: 0.9329 - val_AUC: 0.7757 - val_loss: 0.9153\n",
      "Epoch 27/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7609 - loss: 0.9385 - val_AUC: 0.7761 - val_loss: 0.9141\n",
      "Epoch 28/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7596 - loss: 0.9402 - val_AUC: 0.7759 - val_loss: 0.9139\n",
      "Epoch 29/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7646 - loss: 0.9320 - val_AUC: 0.7763 - val_loss: 0.9138\n",
      "Epoch 30/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7695 - loss: 0.9236 - val_AUC: 0.7766 - val_loss: 0.9131\n",
      "Epoch 31/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7690 - loss: 0.9240 - val_AUC: 0.7768 - val_loss: 0.9119\n",
      "Epoch 32/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7696 - loss: 0.9237 - val_AUC: 0.7775 - val_loss: 0.9113\n",
      "Epoch 33/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - AUC: 0.7672 - loss: 0.9269 - val_AUC: 0.7779 - val_loss: 0.9104\n",
      "Epoch 34/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7730 - loss: 0.9173 - val_AUC: 0.7785 - val_loss: 0.9094\n",
      "Epoch 35/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7697 - loss: 0.9233 - val_AUC: 0.7783 - val_loss: 0.9101\n",
      "Epoch 36/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7725 - loss: 0.9190 - val_AUC: 0.7789 - val_loss: 0.9079\n",
      "Epoch 37/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7752 - loss: 0.9144 - val_AUC: 0.7786 - val_loss: 0.9093\n",
      "Epoch 38/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7730 - loss: 0.9173 - val_AUC: 0.7789 - val_loss: 0.9077\n",
      "Epoch 39/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7744 - loss: 0.9154 - val_AUC: 0.7787 - val_loss: 0.9080\n",
      "Epoch 40/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7692 - loss: 0.9228 - val_AUC: 0.7791 - val_loss: 0.9070\n",
      "Epoch 41/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7758 - loss: 0.9121 - val_AUC: 0.7792 - val_loss: 0.9075\n",
      "Epoch 42/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7719 - loss: 0.9177 - val_AUC: 0.7797 - val_loss: 0.9075\n",
      "Epoch 43/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7786 - loss: 0.9078 - val_AUC: 0.7798 - val_loss: 0.9060\n",
      "Epoch 44/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7816 - loss: 0.9023 - val_AUC: 0.7796 - val_loss: 0.9062\n",
      "Epoch 45/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7780 - loss: 0.9078 - val_AUC: 0.7802 - val_loss: 0.9059\n",
      "Epoch 46/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7777 - loss: 0.9088 - val_AUC: 0.7792 - val_loss: 0.9073\n",
      "Epoch 47/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7804 - loss: 0.9051 - val_AUC: 0.7803 - val_loss: 0.9047\n",
      "Epoch 48/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7782 - loss: 0.9069 - val_AUC: 0.7801 - val_loss: 0.9057\n",
      "Epoch 49/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7811 - loss: 0.9028 - val_AUC: 0.7801 - val_loss: 0.9059\n",
      "Epoch 50/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7809 - loss: 0.9028 - val_AUC: 0.7798 - val_loss: 0.9055\n",
      "Epoch 51/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7764 - loss: 0.9104 - val_AUC: 0.7798 - val_loss: 0.9062\n",
      "Epoch 52/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7791 - loss: 0.9060 - val_AUC: 0.7805 - val_loss: 0.9044\n",
      "Epoch 53/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7787 - loss: 0.9064 - val_AUC: 0.7802 - val_loss: 0.9045\n",
      "Epoch 54/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.7870 - loss: 0.8922 - val_AUC: 0.7802 - val_loss: 0.9049\n",
      "Epoch 55/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7830 - loss: 0.8995 - val_AUC: 0.7801 - val_loss: 0.9045\n",
      "Epoch 56/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7849 - loss: 0.8964 - val_AUC: 0.7798 - val_loss: 0.9049\n",
      "Epoch 57/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7818 - loss: 0.9008 - val_AUC: 0.7805 - val_loss: 0.9042\n",
      "Epoch 58/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7827 - loss: 0.8995 - val_AUC: 0.7810 - val_loss: 0.9046\n",
      "Epoch 59/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7805 - loss: 0.9034 - val_AUC: 0.7805 - val_loss: 0.9048\n",
      "Epoch 60/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7842 - loss: 0.8963 - val_AUC: 0.7810 - val_loss: 0.9042\n",
      "Epoch 61/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7849 - loss: 0.8958 - val_AUC: 0.7811 - val_loss: 0.9039\n",
      "Epoch 62/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7807 - loss: 0.9027 - val_AUC: 0.7810 - val_loss: 0.9032\n",
      "Epoch 63/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7839 - loss: 0.8971 - val_AUC: 0.7809 - val_loss: 0.9042\n",
      "Epoch 64/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7884 - loss: 0.8891 - val_AUC: 0.7807 - val_loss: 0.9031\n",
      "Epoch 65/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7912 - loss: 0.8845 - val_AUC: 0.7808 - val_loss: 0.9041\n",
      "Epoch 66/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7863 - loss: 0.8930 - val_AUC: 0.7803 - val_loss: 0.9041\n",
      "Epoch 67/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7903 - loss: 0.8864 - val_AUC: 0.7807 - val_loss: 0.9049\n",
      "Epoch 68/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7836 - loss: 0.8972 - val_AUC: 0.7809 - val_loss: 0.9039\n",
      "Epoch 69/100\n",
      "\u001b[1m436/436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - AUC: 0.7884 - loss: 0.8895 - val_AUC: 0.7809 - val_loss: 0.9040\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Convert y_train and y_val to categorical\n",
    "y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_val_categorical = tf.keras.utils.to_categorical(y_val, num_classes=3)\n",
    "\n",
    "# Fit the model with the TensorBoard and EarlyStopping callbacks\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val_categorical),\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "172e1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d7214ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.70      0.65      1602\n",
      "           1       0.43      0.24      0.31       860\n",
      "           2       0.61      0.66      0.63      1521\n",
      "\n",
      "    accuracy                           0.58      3983\n",
      "   macro avg       0.55      0.53      0.53      3983\n",
      "weighted avg       0.57      0.58      0.57      3983\n",
      "\n",
      "[[1124  139  339]\n",
      " [ 350  205  305]\n",
      " [ 392  132  997]]\n"
     ]
    }
   ],
   "source": [
    "#use the model to predict the test data, it is a multi-class classification problem with 3 classes\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec6e67f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AUC: 0.7672390559679443\n",
      "1 AUC: 0.6860135603065031\n",
      "2 AUC: 0.7710271738578931\n"
     ]
    }
   ],
   "source": [
    "#write Per-Class ROC-AUC:\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def per_class_roc_auc(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_true)\n",
    "    y_true = lb.transform(y_true)\n",
    "    \n",
    "    for (idx, label) in enumerate(lb.classes_):\n",
    "        print(f\"{label} AUC: {roc_auc_score(y_true[:, idx], y_pred[:, idx])}\")\n",
    "        \n",
    "per_class_roc_auc(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34281f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC Score: 0.3422476863287077\n"
     ]
    }
   ],
   "source": [
    "#mcc score please test\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(y_test, y_pred_labels)\n",
    "\n",
    "print(\"MCC Score:\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'lstm_model.keras'\n"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "model.save('lstm_model.keras')\n",
    "print(\"Model saved to 'lstm_model.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
